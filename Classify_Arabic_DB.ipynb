{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e89c0-4d06-49f7-a875-8a1ef51e7d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded 280 records from file 1\n",
      "‚úÖ Successfully loaded 280 records from file 2\n",
      "üìä First JSON file data shape: (280, 4)\n",
      "üìä Second JSON file data shape: (280, 4)\n",
      "üìè Processing length will be: 280\n",
      "\n",
      "üìã First few rows of first file:\n",
      "   EntryID  Letters_ID  sal_ID word\n",
      "0      572           1       1   ÿ°ÿ≥\n",
      "1      573           1       3   ÿ°ŸÑ\n",
      "2      574           1       4   ÿ°ÿ™\n",
      "3      575           1       5   ÿ°ŸÖ\n",
      "4      576           1       6   ÿ°Ÿà\n",
      "\n",
      "üìã First few rows of second file:\n",
      "   EntryID  Letters_ID  sal_ID word\n",
      "0      852           1       1   ÿ≥ÿ°\n",
      "1      853           2       1   ÿ≥ÿ®\n",
      "2      854           3       1   ÿ≥ÿ™\n",
      "3      855           4       1   ÿ≥ÿ´\n",
      "4      856           5       1   ÿ≥ÿ¨\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # Read JSON data from files automatically\n",
    "# file1_path = r'Data\\JSON_F52E2B61-18A1-11d1-B105-00805F49916B1.txt'\n",
    "# file2_path = r'Data\\new_arabic_json_file.txt'\n",
    "\n",
    "# try:\n",
    "#     # Read first JSON file (handle UTF-8 BOM)\n",
    "#     with open(file1_path, 'r', encoding='utf-8-sig') as f:\n",
    "#         data1 = json.load(f)\n",
    "#     print(f\"‚úÖ Successfully loaded {len(data1)} records from file 1\")\n",
    "    \n",
    "#     # Read second JSON file (also handle UTF-8 BOM)\n",
    "#     with open(file2_path, 'r', encoding='utf-8-sig') as f:\n",
    "#         data2 = json.load(f)\n",
    "#     print(f\"‚úÖ Successfully loaded {len(data2)} records from file 2\")\n",
    "    \n",
    "# except FileNotFoundError as e:\n",
    "#     print(f\"‚ùå Error: Could not find file - {e}\")\n",
    "#     print(\"Please check that the JSON files exist in the Data folder\")\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"‚ùå Error: Invalid JSON format - {e}\")\n",
    "#     print(\"Files may not be valid JSON. Attempting to read as raw text...\")\n",
    "#     try:\n",
    "#         # Fallback: read as text and try to parse\n",
    "#         with open(file1_path, 'r', encoding='utf-8-sig') as f:\n",
    "#             data1 = json.loads(f.read())\n",
    "#         with open(file2_path, 'r', encoding='utf-8-sig') as f:\n",
    "#             data2 = json.loads(f.read())\n",
    "#         print(f\"‚úÖ Successfully loaded with fallback method\")\n",
    "#     except Exception as fallback_error:\n",
    "#         print(f\"‚ùå Fallback also failed: {fallback_error}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Error loading JSON files: {e}\")\n",
    "\n",
    "# # Convert to DataFrames\n",
    "# df1 = pd.DataFrame(data1)\n",
    "# df2 = pd.DataFrame(data2)\n",
    "\n",
    "# print(f\"üìä First JSON file data shape: {df1.shape}\")\n",
    "# print(f\"üìä Second JSON file data shape: {df2.shape}\")\n",
    "\n",
    "# # Determine the maximum length for processing\n",
    "# max_length = max(len(df1), len(df2))\n",
    "# print(f\"üìè Processing length will be: {max_length}\")\n",
    "\n",
    "# # Pad the shorter DataFrame with empty records if needed\n",
    "# if len(df1) < max_length:\n",
    "#     # Create padding records for df1\n",
    "#     padding_df1 = pd.DataFrame([{\"EntryID\": None, \"Letters_ID\": None, \"sal_ID\": None, \"word\": \"\"}] * (max_length - len(df1)))\n",
    "#     df1 = pd.concat([df1, padding_df1], ignore_index=True)\n",
    "#     print(f\"üìù Padded file 1 to {len(df1)} records\")\n",
    "\n",
    "# if len(df2) < max_length:\n",
    "#     # Create padding records for df2  \n",
    "#     padding_df2 = pd.DataFrame([{\"EntryID\": None, \"Letters_ID\": None, \"sal_ID\": None, \"word\": \"\"}] * (max_length - len(df2)))\n",
    "#     df2 = pd.concat([df2, padding_df2], ignore_index=True)\n",
    "#     print(f\"üìù Padded file 2 to {len(df2)} records\")\n",
    "\n",
    "# print(f\"\\nüìã First few rows of first file:\")\n",
    "# print(df1.head())\n",
    "# print(f\"\\nüìã First few rows of second file:\")\n",
    "# print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification groups loaded successfully!\n",
      "Total groups: 25\n",
      "- jar: 99 words\n",
      "- al_jawab: 10 words\n",
      "- istifham: 21 words\n",
      "- jazm: 15 words\n",
      "- shart: 14 words\n",
      "- nidaa: 9 words\n",
      "- ishara: 55 words\n",
      "- mawsoola: 33 words\n",
      "- nasikha: 13 words\n",
      "- ism_feal: 69 words\n",
      "- atef: 9 words\n",
      "- dameer: 34 words\n",
      "- tharf_mabni: 14 words\n",
      "- nafi: 9 words\n",
      "- nasb: 10 words\n",
      "- keniah: 8 words\n",
      "- istidrak: 11 words\n",
      "- al_gaebah: 11 words\n",
      "- idafa: 7 words\n",
      "- taakeed: 18 words\n",
      "- tafseel: 17 words\n",
      "- tafseer_w_taaleel: 16 words\n",
      "- fujaeah: 4 words\n",
      "- tazamon: 10 words\n",
      "- istithnaa: 7 words\n"
     ]
    }
   ],
   "source": [
    "# # Define the classification groups - Updated with expanded groups\n",
    "# groups = [\n",
    "#     (\"jar\", [\n",
    "#         \"ÿ®\", \"ÿ™\", \"ŸÉ\", \"ÿπŸÜ\", \"ŸÅŸä\", \"ŸÖÿ∞\", \"ŸÖŸÜÿ∞\", \"ŸÑ\", \"ŸÖŸÜ\", \"ÿ•ŸÑŸâ\", \"ÿπŸÑŸâ\", \"ÿ±ÿ®\", \"ÿ≠ÿ™Ÿâ\",\n",
    "#         \"ÿÆŸÑÿß\", \"ÿπÿØÿß\", \"ÿ≠ÿßÿ¥ÿß\", \"ŸÉŸä\", \"ŸÑÿπŸÑ\", \"ŸÖÿ™Ÿâ\", \"ŸÑŸàŸÑÿß\",\n",
    "#         \"ŸÖŸÜŸá\",\"ŸÖŸÜŸáÿß\",\"ŸÖŸÜŸáŸÖÿß\",\"ŸÖŸÜŸáŸÖ\",\"ŸÖŸÜŸáŸÜ\",\"ÿπŸÜŸá\",\"ÿπŸÜŸáÿß\",\"ÿπŸÜŸáŸÖÿß\",\"ÿπŸÜŸáŸÖ\",\"ÿπŸÜŸáŸÜ\",\n",
    "#         \"ÿ•ŸÑŸäŸá\",\"ÿ•ŸÑŸäŸáÿß\",\"ÿ•ŸÑŸäŸáŸÖÿß\",\"ÿ•ŸÑŸäŸáŸÜ\",\"ÿπŸÑŸäŸá\",\"ÿπŸÑŸäŸáŸÖÿß\",\"ÿπŸÑŸäŸáÿß\",\"ÿπŸÑŸäŸáŸÖ\",\"ÿπŸÑŸäŸáŸÜ\",\n",
    "#         \"ŸÅŸäŸá\",\"ŸÅŸäŸáÿß\",\"ŸÅŸäŸáŸÖÿß\",\"ŸÅŸäŸáŸÖ\",\"ŸÅŸäŸáŸÜ\",\"ŸÑŸá\",\"ŸÑŸáÿß\",\"ŸÑŸáŸÖÿß\",\"ŸÑŸáŸÖ\",\"ŸÑŸáŸÜ\",\n",
    "#         \"ÿ®Ÿá\",\"ÿ®Ÿáÿß\",\"ÿ®ŸáŸÖÿß\",\"ÿ®ŸáŸÖ\",\"ÿ®ŸáŸÜ\",\n",
    "#         \"ŸÖŸÜŸÉ\",\"ŸÖŸÜŸÉŸÖÿß\",\"ŸÖŸÜŸÉŸÖ\",\"ŸÖŸÜŸÉŸÜ\",\"ÿπŸÜŸÉ\",\"ÿπŸÜŸÉŸÖÿß\",\"ÿπŸÜŸÉŸÜ\",\"ÿπŸÜŸÉŸÖ\",\n",
    "#         \"ÿ•ŸÑŸäŸÉ\",\"ÿ•ŸÑŸäŸÉŸÖÿß\",\"ÿ•ŸÑŸäŸÉŸÖ\",\"ÿ•ŸÑŸäŸÉŸÜ\",\"ÿπŸÑŸäŸÉ\",\"ÿπŸÑŸäŸÉŸÖÿß\",\"ÿπŸÑŸäŸÉŸÖ\",\"ÿπŸÑŸäŸÉŸÜ\",\n",
    "#         \"ŸÅŸäŸÉ\",\"ŸÅŸäŸÉŸÖÿß\",\"ŸÅŸäŸÉŸÖ\",\"ŸÅŸäŸÉŸÜ\",\"ŸÑŸÉ\",\"ŸÑŸÉŸÖÿß\",\"ŸÑŸÉŸÖ\",\"ŸÑŸÉŸÜ\",\"ÿ®ŸÉ\",\"ÿ®ŸÉŸÖÿß\",\"ÿ®ŸÉŸÖ\",\"ÿ®ŸÉŸÜ\",\n",
    "#         \"ŸÖŸÜÿß\",\"ÿπŸÜÿß\",\"ÿ•ŸÑŸäŸÜÿß\",\"ÿπŸÑŸäŸÜÿß\",\"ŸÅŸäŸÜÿß\",\"ÿ®ŸÜÿß\",\"ŸÑŸÜÿß\",\"ŸÖŸÜŸä\",\"ÿπŸÜŸä\",\"ÿπŸÑŸä\",\"ÿ•ŸÑŸä\",\"ŸÑŸä\",\"ÿ®Ÿä\",\n",
    "#         \"ŸàŸÖŸÜ\",\"ŸÅŸÖŸÜ\",\"ŸÑŸÖŸÜ\",\"ŸÉŸÖŸÜ\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"al_jawab\", [\n",
    "#         \"ŸÜÿπŸÖ\", \"ÿ®ŸÑŸâ\", \"ÿ£ÿ¨ŸÑ\", \"ÿ•ŸÜ\", \"ÿ£Ÿä\", \"ÿ£ŸàŸâ\", \"ÿ£ŸÅŸä\", \"ŸÜÿπŸÖÿß\", \"ŸÉŸÑÿß\", \"ÿ®ÿ¨ŸÑ\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"istifham\", [\n",
    "#         \"ŸÖŸÜ\", \"ŸÖŸÜ ÿ∞ÿß\", \"ŸÖÿß\", \"ŸÖÿßÿ∞ÿß\", \"ŸÖÿ™Ÿâ\", \"ÿ£ŸäÿßŸÜ\", \"ÿ£ŸäŸÜ\", \"ÿ£ŸÜŸâ\", \"ŸÉŸÖ\", \"ŸÉŸäŸÅ\",\n",
    "#         \"ÿ£\", \"ÿ£ÿ£\", \"ŸÅŸäŸÖ\",\"ÿπŸÖ\",\"ŸÖŸÖ\",\"ÿ®ŸÖ\",\"ŸÑŸÖ\",\"ÿ•ŸÑÿßŸÖ\",\"ÿ≠ÿ™ÿßŸÖ\",\"ÿπŸÑÿßŸÖ\", \"ŸáŸÑ\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"jazm\", [\n",
    "#         \"ŸÑŸÖ\", \"ŸÑŸÖÿß\", \"ŸÑÿß\", \"ŸÑ\", \"ÿ•ŸÜ\", \"ÿ•ŸÜŸÖÿß\", \"ŸÖŸÜ\", \"ŸÖÿß\", \"ŸÖŸáŸÖÿß\", \"ŸÖÿ™Ÿâ\", \"ÿ£ŸäÿßŸÜ\", \"ÿ£ŸäŸÜ\",\n",
    "#         \"ÿ≠Ÿäÿ´ŸÖÿß\", \"ÿ£ŸÜŸâ\", \"ÿßÿ∞ŸÖÿß\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"shart\", [\n",
    "#         \"ŸÖŸÜ\", \"ÿ•ŸÜ\", \"ÿ£ŸäŸÜ\", \"ÿ£ŸäŸÜŸÖÿß\", \"ŸÉŸäŸÅŸÖÿß\", \"ŸÖÿ™Ÿâ\", \"ÿ≠Ÿäÿ´ŸÖÿß\", \"ÿ£ŸÜŸâ\", \"ÿ£ŸäÿßŸÜ\", \"ŸÖÿß\", \"ŸÖŸáŸÖÿß\",\n",
    "#         \"ÿßÿ∞ŸÖÿß\", \"ŸÑŸà\", \"ÿ•ÿ∞ÿß\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"nidaa\", [\n",
    "#         \"Ÿäÿß\", \"ÿ£Ÿäÿß\", \"ŸáŸäÿß\", \"ÿ£Ÿä\", \"ÿ¢Ÿä\", \"ÿ£ŸäŸáÿß\", \"Ÿàÿß\", \"ÿ¢\", \"ÿ£\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"ishara\", [\n",
    "#         \"Ÿáÿ∞ÿß\", \"Ÿáÿ∞Ÿá\", \"Ÿáÿ∞Ÿä\", \"ŸáÿßŸáŸÜÿß\", \"ÿ∞ÿßŸÉ\", \"ÿ™ŸÑŸÉ\", \"ÿ™ŸäŸÉ\", \"ÿ∞ŸÑŸÉ\", \"ŸáŸÜÿß\", \"ŸáŸÜÿßŸÑŸÉ\",\n",
    "#         \"Ÿáÿ§ŸÑÿßÿ°\", \"ÿßŸàŸÑÿ¶ŸÉ\", \"Ÿáÿ∞ÿßŸÜ\", \"Ÿáÿ™ÿßŸÜ\", \"ŸàŸáÿ∞ÿß\", \"ŸàŸáÿ∞Ÿá\", \"ŸàŸáÿ∞ÿßŸÜ\", \"ŸàŸáÿßÿ™ÿßŸÜ\", \"ŸàŸáÿ§ŸÑÿßÿ°\",\n",
    "#         \"ŸÅŸáÿ∞ÿß\", \"ŸÅŸáÿ∞Ÿá\", \"ŸÅŸáÿ∞ÿßŸÜ\", \"ŸÅŸáÿßÿ™ÿßŸÜ\", \"ŸÅŸáÿ§ŸÑÿßÿ°\", \"ŸÑŸáÿ∞ÿß\", \"ŸÑŸáÿ∞Ÿá\", \"ŸÑŸáÿ∞ŸäŸÜ\", \"ŸÑŸáÿßÿ™ŸäŸÜ\",\n",
    "#         \"ŸÑŸáÿ§ŸÑÿßÿ°\", \"ÿ®Ÿáÿ∞ÿß\", \"ÿ®Ÿáÿ∞Ÿá\", \"ÿ®Ÿáÿ∞ŸäŸÜ\", \"ÿ®Ÿáÿßÿ™ŸäŸÜ\", \"ÿ®Ÿáÿ§ŸÑÿßÿ°\", \"ŸÉŸáÿ∞ÿß\", \"ŸÉŸáÿ∞Ÿá\", \"ŸÉŸáÿ∞ÿßŸÜ\",\n",
    "#         \"ŸÉŸáÿßÿ™ÿßŸÜ\", \"ŸÉŸáÿ§ŸÑÿßÿ°\", \"Ÿàÿ∞ŸÑŸÉ\", \"Ÿàÿ™ŸÑŸÉ\", \"ŸÅÿ∞ŸÑŸÉ\", \"ŸÅÿ™ŸÑŸÉ\", \"ŸÑÿ∞ŸÑŸÉ\", \"ŸÑÿ™ŸÑŸÉ\", \"ÿ®ÿ∞ŸÑŸÉ\",\n",
    "#         \"ÿ®ÿ™ŸÑŸÉ\", \"ŸÉÿ∞ŸÑŸÉ\", \"ŸÉÿ™ŸÑŸÉ\", \"ÿ∞ŸÑŸÉŸÖÿß\", \"ÿ∞ŸÑŸÉŸÖ\", \"ÿ∞ŸÑŸÉŸÜ\", \"ÿ™ŸÑŸÉŸÖÿß\", \"ÿ™ŸÑŸÉŸÖ\", \"ÿ™ŸÑŸÉŸÜ\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"mawsoola\", [\n",
    "#         \"ÿßŸÑÿ∞Ÿä\", \"ÿßŸÑÿ™Ÿä\", \"ÿßŸÑÿ∞ŸäŸÜ\", \"ÿßŸÑŸÑÿßÿ™Ÿä\", \"ÿßŸÑŸÑÿßÿ¶Ÿä\", \"ŸÖŸÜ\", \"ŸÖÿß\", \"ÿ∞ÿß\", \"ŸàÿßŸÑÿ∞Ÿä\",\n",
    "#         \"ŸàÿßŸÑÿ™Ÿä\",\"ŸàÿßŸÑŸÑÿ∞ÿßŸÜ\",\"ŸàÿßŸÑŸÑÿ™ÿßŸÜ\",\"ŸàÿßŸÑÿ∞ŸäŸÜ\",\"ŸÅÿßŸÑÿ∞Ÿä\",\"ŸÅÿßŸÑÿ™Ÿä\",\"ŸÅÿßŸÑŸÑÿ∞ÿßŸÜ\",\"ŸÅÿßŸÑŸÑÿ™ÿßŸÜ\",\"ŸÅÿßŸÑÿ∞ŸäŸÜ\",\n",
    "#         \"ŸÑŸÑÿ∞Ÿä\",\"ŸÑŸÑÿ™Ÿä\",\"ŸÑŸÑŸÑÿ∞ŸäŸÜ\",\"ŸÑŸÑŸÑÿ™ŸäŸÜ\",\"ŸÑŸÑÿ∞ŸäŸÜ\",\"ÿ®ÿßŸÑÿ∞Ÿä\",\"ÿ®ÿßŸÑÿ™Ÿä\",\"ÿ®ÿßŸÑŸÑÿ∞ŸäŸÜ\",\"ÿ®ÿßŸÑŸÑÿ™ŸäŸÜ\",\"ÿ®ÿßŸÑÿ∞ŸäŸÜ\",\n",
    "#         \"ŸÉÿßŸÑÿ∞Ÿä\",\"ŸÉÿßŸÑÿ™Ÿä\",\"ŸÉÿßŸÑŸÑÿ∞ŸäŸÜ\",\"ŸÉÿßŸÑŸÑÿ™ŸäŸÜ\",\"ŸÉÿßŸÑÿ∞ŸäŸÜ\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"nasikha\", [\n",
    "#         \"ÿ•ŸÜ\", \"ÿ£ŸÜ\", \"ŸÉÿ£ŸÜ\", \"ŸÑŸÉŸÜ\", \"ŸÑŸäÿ™\", \"ŸÑÿπŸÑ\", \"ŸÑÿß\", \"ÿ•ŸÜŸÖÿß\", \"ÿ£ŸÜŸÖÿß\", \"ŸÑŸäÿ™ŸÖÿß\", \"ŸÉÿ£ŸÜŸÖÿß\",\n",
    "#         \"ŸÑÿπŸÑŸÖÿß\", \"ŸÑŸÉŸÜŸÖÿß\"\n",
    "#     ]),\n",
    "\n",
    "#     # ÿßŸÑŸÇŸàÿßÿ¶ŸÖ ÿßŸÑÿ¨ÿØŸäÿØÿ©\n",
    "#     (\"ism_feal\", [\n",
    "#         \"ÿ¢ŸÖŸäŸÜ\", \"ÿ≠Ÿä\", \"ŸáŸäÿß\", \"ŸáŸäŸÉ\", \"ŸáŸäÿ™\", \"ŸáŸÑŸÖ ÿßŸÑŸâ\", \"ŸáŸÑŸÖ ŸÉÿ∞ÿß\", \"Ÿáÿßÿ§ŸÖ\", \"ÿµŸá\",\n",
    "#         \"ÿ•ŸäŸá\", \"ŸÖŸá\", \"ÿ±ŸàŸäÿØÿß\", \"ÿ±ŸàŸäÿØŸÉ\", \"ÿ®ŸÑŸá\", \"Ÿáÿßÿ°\", \"ŸáÿßŸÉ\", \"ŸáÿßŸÉŸÖ\", \"ÿØŸàŸÜŸÉ\", \"ÿ≠ÿ≥\", \"ÿ™ŸäÿØ\",\n",
    "#         \"ŸáŸäŸáŸÑ\", \"ŸáŸäŸáŸÑŸÉ\", \"ŸáŸäŸáŸÑÿß\", \"ÿ≠ÿØŸäÿßŸÉ\", \"ÿ®ÿ≥\", \"ÿßŸÑŸÜÿ¨ÿßÿ°ŸÉ\", \"ÿ•ŸäŸáÿß\", \"ŸÅÿØÿßÿ°\", \"ŸÇÿØŸÉ\", \"ŸÇÿ∑ŸÉ\",\n",
    "#         \"ŸÉÿ∞ÿßŸÉ\", \"ŸáÿØÿßÿØŸäŸÉ\", \"ŸÑÿØŸäŸÉ\", \"ÿπŸÜÿØŸÉ\", \"ŸàŸäŸáÿß\", \"ŸàŸäŸá\", \"Ÿàÿ±ÿßÿ°ŸÉ\", \"ÿ•ŸÑŸäŸÉ\", \"ÿπŸÜ\", \"ÿ•ŸÑŸä\", \n",
    "#         \"ÿπŸÑŸäŸÉ\", \"ŸÖŸÉÿßŸÜŸÉ\", \"ÿ£ŸÖÿßŸÖŸÉ\", \"ÿ≠ÿ∞ÿßÿ±\", \"ŸÜÿ≤ÿßŸÑ\", \"ŸáŸäŸáÿßÿ™\", \"ÿ®ÿ∑ÿ¢ŸÜ\", \"ÿ®ÿ∑ÿ¶ÿßŸÜ\", \"ÿ¥ÿ™ÿßŸÜ\", \n",
    "#         \"ÿ≥ÿ±ÿπÿßŸÜ\", \"Ÿàÿ¥ŸÉÿßŸÜ\", \"ÿØŸáÿØÿ±ŸäŸäŸÜ\", \"ŸÇÿØ\", \"ÿ≠ÿ≥ÿ®\", \"ÿ≠ÿ≥ÿ®ŸÉ\", \"ÿ®ÿ¨ŸÑ\", \"ÿ£ŸÅ\", \"ŸàŸä\", \"ŸàŸäŸÉ\", \n",
    "#         \"ŸàÿßŸá\", \"ŸàÿßŸáÿß\", \"ÿ¢Ÿá\", \"ÿ¢Ÿáÿß\", \"ÿ£ŸàÿßŸá\", \"ÿ£ŸàŸá\", \"ŸÇÿ∑\", \"ÿ®ÿÆ\", \"ÿ≤Ÿá\", \"ÿ£ÿÆ\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"atef\", [\"Ÿà\", \"ŸÅ\", \"ÿ´ŸÖ\", \"ÿ≠ÿ™Ÿâ\", \"ÿßŸà\", \"ÿßŸÖ\", \"ÿ®ŸÑ\", \"ŸÑŸÉŸÜ\", \"ŸÑÿß\"]),\n",
    "\n",
    "#     (\"dameer\", [\n",
    "#         # ÿ∂ŸÖÿßÿ¶ÿ± ÿßŸÑÿ±ŸÅÿπ ÿßŸÑŸÖŸÜŸÅÿµŸÑÿ©\n",
    "#         \"ÿßŸÜÿß\",\"ŸÜÿ≠ŸÜ\",\"ÿßŸÜÿ™\",\"ÿßŸÜÿ™\",\"ÿßŸÜÿ™ŸÖÿß\",\"ÿßŸÜÿ™ŸÖ\",\"ÿßŸÜÿ™ŸÜ\",\"ŸáŸà\",\"ŸáŸä\",\"ŸáŸÖÿß\",\"ŸáŸÖ\",\"ŸáŸÜ\",\n",
    "#         # ÿ∂ŸÖÿßÿ¶ÿ± ÿßŸÑŸÜÿµÿ® ÿßŸÑŸÖŸÜŸÅÿµŸÑÿ©\n",
    "#         \"ÿßŸäÿßŸä\",\"ÿßŸäÿßŸÜÿß\",\"ÿßŸäÿßŸÉ\",\"ÿßŸäÿßŸÉ\",\"ÿßŸäÿßŸÉŸÖÿß\",\"ÿßŸäÿßŸÉŸÖ\",\"ÿßŸäÿßŸÉŸÜ\",\"ÿßŸäÿßŸá\",\"ÿßŸäÿßŸáÿß\",\"ÿßŸäÿßŸáŸÖÿß\",\"ÿßŸäÿßŸáŸÜ\",\n",
    "#         # ÿ∂ŸÖÿßÿ¶ÿ± ÿßŸÑÿ±ŸÅÿπ ÿßŸÑŸÖÿ™ÿµŸÑÿ©\n",
    "#         \"ÿ™\", \"ŸÜÿß\", \"ŸÜ\", \"ÿß\", \"Ÿàÿß\", \"Ÿä\",\n",
    "#         # ÿ∂ŸÖÿßÿ¶ÿ± ÿßŸÑŸÜÿµÿ® ÿßŸÑŸÖÿ™ÿµŸÑÿ©\n",
    "#         \"ŸÉŸÖ\", \"Ÿá\", \"ŸÉ\", \"Ÿä\", \"ŸÜÿß\"\n",
    "#     ]),\n",
    "\n",
    "#     (\"tharf_mabni\", [\"ÿ≠Ÿäÿ´\",\"ŸÖŸÜÿ∞\",\"ŸÇÿ∑\",\"ÿßŸÑÿßŸÜ\",\"ÿ∫ÿØÿß\",\"ÿßŸÑŸÑŸäŸÑÿ©\",\"ÿ´ŸÖ\",\"ÿ£ŸäÿßŸÜ\",\"ŸáŸÜÿßŸÉ\",\"ŸáŸÜÿßŸÑŸÉ\",\"ÿ´ÿ©\",\"ÿßÿ∞\",\"ŸÖÿ∞\",\"ŸÑÿØŸâ\"]),\n",
    "\n",
    "#     (\"nafi\", [\"ŸÖÿß\",\"ŸÑÿß\",\"ÿ•ŸÜ\",\"ŸÑŸÖ\",\"ŸÑŸÖÿß\",\"ŸÑŸÜ\",\"ŸÑŸäÿ≥\",\"ŸÑÿßÿ™\",\"ÿ∫Ÿäÿ±\"]),\n",
    "\n",
    "#     (\"nasb\", [\"ÿ£ŸÜ\",\"ŸÑŸÜ\",\"ÿ•ÿ∞ŸÜ\",\"ŸÉŸä\",\"ÿßŸÑŸÑÿßŸÖ\",\"ŸÑÿßŸÖ\",\"ÿ≠ÿ™Ÿâ\",\"ŸÅÿßÿ°\",\"ŸàÿßŸà\",\"ÿ£Ÿà\"]),\n",
    "\n",
    "#     (\"keniah\", [\"ŸÉŸÖ\",\"ŸÉÿ£ŸäŸÜ\",\"ŸÉÿßÿ¶ŸÜ\",\"ŸÉÿ£Ÿä\",\"ŸÉÿ∞ÿß\",\"ŸÉŸäÿ™\",\"ÿ∞Ÿäÿ™\",\"ÿ®ÿ∂ÿπ\"]),\n",
    "\n",
    "#     (\"istidrak\", [\"ŸÑŸÉŸÜ\",\"ÿ®ŸÑ\",\"ÿ®ŸäÿØ ÿßŸÜ\",\"ÿ•ŸÑÿß ÿßŸÜ\",\"ÿ∫Ÿäÿ± ÿßŸÜ\",\"ÿπŸÑŸâ ÿßŸÑÿ±ÿ∫ŸÖ ŸÖŸÜ\",\"ÿπŸÑŸâ ÿ£Ÿäÿ© ÿ≠ÿßŸÑ\",\"ÿπŸÑŸÖÿß ÿ£ŸÜ\",\"ŸÑÿ∞ŸÑŸÉ\",\"ÿπŸÑŸâ ÿ£ŸÜ\",\"ŸÖÿπ ÿ∞ŸÑŸÉ\"]),\n",
    "\n",
    "#     (\"al_gaebah\", [\"ŸÑÿ∫ÿßŸäŸá\",\"ŸÑŸáÿØŸÅ\",\"ŸÑÿ£ÿ¨ŸÑ\",\"ŸàÿµŸàŸÑÿß ÿßŸÑŸâ\",\"ŸÖŸÜ ÿßÿ¨ŸÑ\",\"ŸÜÿ™Ÿäÿ¨Ÿá ŸÑ\",\"ŸÑŸáÿ∞ÿß\",\"ŸÉŸä\",\"ÿ±ÿ∫ÿ®ÿ© ŸÅŸä\",\"ÿπŸÑŸâ Ÿáÿ∞ÿß\",\"ŸàÿπŸÑŸäŸá\"]),\n",
    "\n",
    "#     (\"idafa\", [\"ÿ®ÿßŸÑÿßÿ∂ÿßŸÅÿ© ÿßŸÑŸâ\",\"ŸÇÿ≥ ÿπŸÑŸâ ÿ∞ŸÑŸÉ\",\"ÿßŸÑŸâ ÿ¨ÿßŸÜÿ® ÿ∞ŸÑŸÉ\",\"ŸÉŸÖÿß\",\"Ÿàÿ£Ÿäÿ∂ÿß\",\"ŸÉÿ∞ŸÑŸÉ\",\"ŸÅÿ∂ŸÑÿß ÿπŸÜ\"]),\n",
    "\n",
    "#     (\"taakeed\", [\"ŸÉŸÑÿß\",\"ŸÉŸÑÿ™ÿß\",\"ÿ¨ŸÖŸäÿπ\",\"ÿπŸäŸÜ\",\"ŸÜŸÅÿ≥\",\"ŸÉÿßŸÅÿ©\",\"ÿπÿßŸÖŸá\",\"ŸÇÿØ\",\"ÿ£ŸÜ\",\"ŸÉŸÑ\",\"ÿ£ÿ¨ŸÖÿπ\",\"ÿ¨ŸÖÿπÿßÿ°\",\"ÿ£ÿ¨ŸÖÿπŸàŸÜ\",\"ÿ¨ŸÖÿπ\",\"ÿ•ŸÜ\",\"ŸÑ\",\"ŸÜ\",\"ŸáŸÑÿß\"]),\n",
    "\n",
    "#     (\"tafseel\", [\"ÿ£ŸÖÿß\",\"ÿ•ŸÖÿß\",\"ÿßŸÑŸàÿßŸà\",\"ÿ´ŸÖ\",\"ÿ•ÿ∞ŸÜ\",\"ÿ∫Ÿäÿ± ÿ£ŸÜ\",\"ÿ£Ÿä\",\"ÿßŸÑŸÅÿßÿ°\",\"ŸÑÿß ÿ≥ŸäŸÖÿß\",\"ÿÆÿµŸàÿµÿß\",\"ÿ®ÿÆÿßÿµÿ©\",\"ÿ®ŸÑ\",\"ŸÖÿ´ŸÑ\",\"ÿ£ÿπŸÜŸä\",\"ÿ£ŸÇÿµÿØ\",\"ŸÖŸÜŸáÿß\",\"ŸÖŸÜ ÿ∞ŸÑŸÉ\"]),\n",
    "\n",
    "#     (\"tafseer_w_taaleel\", [\"ÿ£Ÿä\",\"ÿßŸÑŸÅÿßÿ°\",\"ŸÑ\",\"ŸÅŸÇÿØ\",\"ŸÑÿ£ŸÜ\",\"ÿ®ÿ≥ÿ®ÿ®\",\"ÿ±ÿßÿ¨ÿπ ÿßŸÑŸâ\",\"ŸÑŸÉŸä\",\"ŸÑÿ£ÿ¨ŸÑ ÿ∞ŸÑŸÉ\",\"ÿ®ŸÜÿßÿ° ÿπŸÑŸâ\",\"ŸÉŸä\",\"ŸÑÿ£ŸÜ\",\"ŸÑÿ≥ÿ®ÿ®\",\"ŸÜÿ∏ÿ±ÿß ŸÑŸÄ\",\"ŸÑŸáÿ∞ÿß\",\"ÿßÿ∞\"]),\n",
    "\n",
    "#     (\"fujaeah\", [\"ÿßÿ∞ÿß\",\"ÿßÿ∞\",\"ŸÅÿßÿ∞ÿß\",\"Ÿàÿßÿ∞ÿß\"]),\n",
    "\n",
    "#     (\"tazamon\", [\"ÿ®ŸäŸÜŸÖÿß\",\"ÿ™ÿ≤ÿßŸÖŸÜÿß\",\"ÿ≠ŸäŸÜŸáÿß\",\"ÿ≥ÿßÿπÿ™Ÿáÿß\",\"ÿ™ÿ≤ÿßŸÖŸÜÿß ŸÖÿπ\",\"ÿ®ŸäŸÜÿß\",\"ŸÑŸÖÿß\",\"ÿßÿ´ŸÜÿßÿ°\",\"ÿÆŸÑÿßŸÑ\",\"ŸÖÿπ\"]),\n",
    "\n",
    "#     (\"istithnaa\", [\"ÿßŸÑÿß\",\"ÿ∫Ÿäÿ±\",\"ÿ≥ŸàŸâ\",\"ÿÆŸÑÿß\",\"ÿπÿØÿß\",\"ÿ≠ÿßÿ¥ÿß\",\"ŸÑŸäÿ≥\"])\n",
    "# ]\n",
    "\n",
    "# # Function to classify a word\n",
    "# def classify_word(word):\n",
    "#     for group_name, words in groups:\n",
    "#         if word in words:\n",
    "#             return group_name\n",
    "#     return \"no_match\"\n",
    "\n",
    "# # Function to determine if word has meaning\n",
    "# def has_meaning(classification):\n",
    "#     return classification != \"no_match\"\n",
    "\n",
    "# print(\"Classification groups loaded successfully!\")\n",
    "# print(\"Total groups:\", len(groups))\n",
    "# for group_name, words in groups:\n",
    "#     print(f\"- {group_name}: {len(words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a803775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final DataFrame created with SEPARATED columns!\n",
      "üìä Shape: (280, 6)\n",
      "üìã Columns: ['File1_Word', 'File1_Classification', 'File1_Meaning', 'File2_Word', 'File2_Classification', 'File2_Meaning']\n",
      "\n",
      "üìã First 20 rows:\n",
      "File1_Word File1_Classification  File1_Meaning File2_Word File2_Classification  File2_Meaning\n",
      "        ÿ°ÿ≥             no_match          False         ÿ≥ÿ°             no_match          False\n",
      "        ÿ°ŸÑ             no_match          False         ÿ≥ÿ®             no_match          False\n",
      "        ÿ°ÿ™             no_match          False         ÿ≥ÿ™             no_match          False\n",
      "        ÿ°ŸÖ             no_match          False         ÿ≥ÿ´             no_match          False\n",
      "        ÿ°Ÿà             no_match          False         ÿ≥ÿ¨             no_match          False\n",
      "        ÿ°ŸÜ             no_match          False         ÿ≥ÿ≠             no_match          False\n",
      "        ÿ°Ÿä             no_match          False         ÿ≥ÿÆ             no_match          False\n",
      "        ÿ°Ÿá             no_match          False         ÿ≥ÿØ             no_match          False\n",
      "        ÿ°ÿß             no_match          False         ÿ≥ÿ∞             no_match          False\n",
      "        ÿ®ÿ≥             ism_feal           True         ÿ≥ÿ±             no_match          False\n",
      "        ÿ®ÿ°             no_match          False         ÿ≥ÿ≤             no_match          False\n",
      "        ÿ®ŸÑ                 atef           True         ÿ≥ÿ¥             no_match          False\n",
      "        ÿ®ÿ™             no_match          False         ÿ≥ÿµ             no_match          False\n",
      "        ÿ®ŸÖ             istifham           True         ÿ≥ÿ∂             no_match          False\n",
      "        ÿ®Ÿà             no_match          False         ÿ≥ÿπ             no_match          False\n",
      "        ÿ®ŸÜ             no_match          False         ÿ≥ÿ∫             no_match          False\n",
      "        ÿ®Ÿä                  jar           True         ÿ≥ŸÅ             no_match          False\n",
      "        ÿ®Ÿá                  jar           True         ÿ≥ŸÇ             no_match          False\n",
      "        ÿ®ÿß             no_match          False         ÿ≥ŸÉ             no_match          False\n",
      "        ÿ™ÿ≥             no_match          False         ÿ≥ŸÑ             no_match          False\n"
     ]
    }
   ],
   "source": [
    "# # Create the final classification DataFrame with SEPARATED columns\n",
    "# # Each file will have its own classification and meaning columns\n",
    "\n",
    "# # Create lists to store data for the final DataFrame - SEPARATED FORMAT\n",
    "# file1_words = []\n",
    "# file1_classifications = []\n",
    "# file1_meanings = []\n",
    "# file2_words = []\n",
    "# file2_classifications = []\n",
    "# file2_meanings = []\n",
    "\n",
    "# # Process both files with separated classification\n",
    "# for i in range(len(df1)):\n",
    "#     word1 = df1.iloc[i]['word'] if i < len(df1) else \"\"\n",
    "#     word2 = df2.iloc[i]['word'] if i < len(df2) else \"\"\n",
    "    \n",
    "#     file1_words.append(word1)\n",
    "#     file2_words.append(word2)\n",
    "    \n",
    "#     # Classify each word SEPARATELY\n",
    "#     class1 = classify_word(word1)\n",
    "#     class2 = classify_word(word2)\n",
    "    \n",
    "#     # File 1 classification and meaning\n",
    "#     if class1 != \"no_match\":\n",
    "#         file1_classifications.append(class1)  # NO File1: prefix\n",
    "#         file1_meanings.append(True)\n",
    "#     else:\n",
    "#         file1_classifications.append(\"no_match\")\n",
    "#         file1_meanings.append(False)\n",
    "    \n",
    "#     # File 2 classification and meaning  \n",
    "#     if class2 != \"no_match\":\n",
    "#         file2_classifications.append(class2)  # NO File2: prefix\n",
    "#         file2_meanings.append(True)\n",
    "#     else:\n",
    "#         file2_classifications.append(\"no_match\")\n",
    "#         file2_meanings.append(False)\n",
    "\n",
    "# # Create the final DataFrame with SEPARATED columns\n",
    "# final_df = pd.DataFrame({\n",
    "#     'File1_Word': file1_words,\n",
    "#     'File1_Classification': file1_classifications,\n",
    "#     'File1_Meaning': file1_meanings,\n",
    "#     'File2_Word': file2_words,\n",
    "#     'File2_Classification': file2_classifications,\n",
    "#     'File2_Meaning': file2_meanings\n",
    "# })\n",
    "\n",
    "# print(\"‚úÖ Final DataFrame created with SEPARATED columns!\")\n",
    "# print(\"üìä Shape:\", final_df.shape)\n",
    "# print(\"üìã Columns:\", list(final_df.columns))\n",
    "# print(\"\\nüìã First 20 rows:\")\n",
    "# print(final_df.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80120507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ CLASSIFICATION RESULTS SUMMARY - SEPARATED FORMAT\n",
      "======================================================================\n",
      "\n",
      "üìÅ FILE 1 STATISTICS:\n",
      "   ‚úÖ Words with meaning: 37\n",
      "   ‚ùå Words without meaning: 243\n",
      "   üìÇ Classifications:\n",
      "      ‚Ä¢ jar: 8 words\n",
      "      ‚Ä¢ ism_feal: 7 words\n",
      "      ‚Ä¢ istifham: 6 words\n",
      "      ‚Ä¢ dameer: 5 words\n",
      "      ‚Ä¢ atef: 4 words\n",
      "      ‚Ä¢ nidaa: 2 words\n",
      "      ‚Ä¢ mawsoola: 1 words\n",
      "      ‚Ä¢ shart: 1 words\n",
      "      ‚Ä¢ taakeed: 1 words\n",
      "      ‚Ä¢ jazm: 1 words\n",
      "      ‚Ä¢ nafi: 1 words\n",
      "\n",
      "üìÅ FILE 2 STATISTICS:\n",
      "   ‚úÖ Words with meaning: 24\n",
      "   ‚ùå Words without meaning: 256\n",
      "   üìÇ Classifications:\n",
      "      ‚Ä¢ jar: 5 words\n",
      "      ‚Ä¢ dameer: 5 words\n",
      "      ‚Ä¢ istifham: 3 words\n",
      "      ‚Ä¢ nidaa: 2 words\n",
      "      ‚Ä¢ ism_feal: 2 words\n",
      "      ‚Ä¢ atef: 2 words\n",
      "      ‚Ä¢ nafi: 1 words\n",
      "      ‚Ä¢ tazamon: 1 words\n",
      "      ‚Ä¢ jazm: 1 words\n",
      "      ‚Ä¢ shart: 1 words\n",
      "      ‚Ä¢ tharf_mabni: 1 words\n",
      "\n",
      "======================================================================\n",
      "üí° EXAMPLES OF CLASSIFIED WORDS\n",
      "======================================================================\n",
      "\n",
      "üìÅ FROM FILE 1:\n",
      "   Row 10: 'ÿ®ÿ≥' ‚Üí ism_feal\n",
      "   Row 12: 'ÿ®ŸÑ' ‚Üí atef\n",
      "   Row 14: 'ÿ®ŸÖ' ‚Üí istifham\n",
      "   Row 17: 'ÿ®Ÿä' ‚Üí jar\n",
      "   Row 18: 'ÿ®Ÿá' ‚Üí jar\n",
      "   Row 33: 'ÿ´ŸÖ' ‚Üí atef\n",
      "   Row 49: 'ÿ≠ÿ≥' ‚Üí ism_feal\n",
      "   Row 56: 'ÿ≠Ÿä' ‚Üí ism_feal\n",
      "   Row 88: 'ÿ∞ÿß' ‚Üí mawsoola\n",
      "   Row 107: 'ÿ≤Ÿá' ‚Üí ism_feal\n",
      "\n",
      "üìÅ FROM FILE 2:\n",
      "   Row 76: 'ŸÑŸÉ' ‚Üí jar\n",
      "   Row 77: 'ŸÑŸÖ' ‚Üí istifham\n",
      "   Row 78: 'ŸÑŸÜ' ‚Üí nafi\n",
      "   Row 79: 'ŸÑŸá' ‚Üí jar\n",
      "   Row 80: 'ŸÑŸà' ‚Üí shart\n",
      "   Row 81: 'ŸÑŸä' ‚Üí jar\n",
      "   Row 82: 'ŸÑÿß' ‚Üí jazm\n",
      "   Row 121: 'ŸÖÿ∞' ‚Üí jar\n",
      "   Row 128: 'ŸÖÿπ' ‚Üí tazamon\n",
      "   Row 134: 'ŸÖŸÜ' ‚Üí jar\n",
      "\n",
      "üìä COMBINED STATISTICS BY GROUP:\n",
      "   ‚Ä¢ jar: 13 words (File1: 8, File2: 5)\n",
      "   ‚Ä¢ istifham: 9 words (File1: 6, File2: 3)\n",
      "   ‚Ä¢ jazm: 2 words (File1: 1, File2: 1)\n",
      "   ‚Ä¢ shart: 2 words (File1: 1, File2: 1)\n",
      "   ‚Ä¢ nidaa: 4 words (File1: 2, File2: 2)\n",
      "   ‚Ä¢ mawsoola: 1 words (File1: 1, File2: 0)\n",
      "   ‚Ä¢ ism_feal: 9 words (File1: 7, File2: 2)\n",
      "   ‚Ä¢ atef: 6 words (File1: 4, File2: 2)\n",
      "   ‚Ä¢ dameer: 10 words (File1: 5, File2: 5)\n",
      "   ‚Ä¢ tharf_mabni: 1 words (File1: 0, File2: 1)\n",
      "   ‚Ä¢ nafi: 2 words (File1: 1, File2: 1)\n",
      "   ‚Ä¢ taakeed: 1 words (File1: 1, File2: 0)\n",
      "   ‚Ä¢ tazamon: 1 words (File1: 0, File2: 1)\n",
      "\n",
      "üéØ FINAL TOTALS:\n",
      "   üìä Total meaningful words: 61\n",
      "   üìä Total words processed: 280\n",
      "   üìà Percentage with meaning: 21.79%\n"
     ]
    }
   ],
   "source": [
    "# # Show detailed statistics with SEPARATED columns\n",
    "# print(\"=\"*70)\n",
    "# print(\"üéØ CLASSIFICATION RESULTS SUMMARY - SEPARATED FORMAT\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Statistics for File 1\n",
    "# file1_meaningful = final_df[final_df['File1_Meaning'] == True]\n",
    "# file1_classifications = file1_meaningful['File1_Classification'].value_counts()\n",
    "\n",
    "# print(f\"\\nüìÅ FILE 1 STATISTICS:\")\n",
    "# print(f\"   ‚úÖ Words with meaning: {len(file1_meaningful)}\")\n",
    "# print(f\"   ‚ùå Words without meaning: {len(final_df[final_df['File1_Meaning'] == False])}\")\n",
    "# if len(file1_classifications) > 0:\n",
    "#     print(f\"   üìÇ Classifications:\")\n",
    "#     for classification, count in file1_classifications.items():\n",
    "#         print(f\"      ‚Ä¢ {classification}: {count} words\")\n",
    "\n",
    "# # Statistics for File 2  \n",
    "# file2_meaningful = final_df[final_df['File2_Meaning'] == True]\n",
    "# file2_classifications = file2_meaningful['File2_Classification'].value_counts()\n",
    "\n",
    "# print(f\"\\nüìÅ FILE 2 STATISTICS:\")\n",
    "# print(f\"   ‚úÖ Words with meaning: {len(file2_meaningful)}\")\n",
    "# print(f\"   ‚ùå Words without meaning: {len(final_df[final_df['File2_Meaning'] == False])}\")\n",
    "# if len(file2_classifications) > 0:\n",
    "#     print(f\"   üìÇ Classifications:\")\n",
    "#     for classification, count in file2_classifications.items():\n",
    "#         print(f\"      ‚Ä¢ {classification}: {count} words\")\n",
    "\n",
    "# # Show examples of classified words from both files\n",
    "# print(f\"\\n\" + \"=\"*70)\n",
    "# print(\"üí° EXAMPLES OF CLASSIFIED WORDS\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# print(f\"\\nüìÅ FROM FILE 1:\")\n",
    "# for idx, row in file1_meaningful.head(10).iterrows():\n",
    "#     print(f\"   Row {idx+1}: '{row['File1_Word']}' ‚Üí {row['File1_Classification']}\")\n",
    "\n",
    "# print(f\"\\nüìÅ FROM FILE 2:\")\n",
    "# for idx, row in file2_meaningful.head(10).iterrows():\n",
    "#     print(f\"   Row {idx+1}: '{row['File2_Word']}' ‚Üí {row['File2_Classification']}\")\n",
    "\n",
    "# # Combined statistics by group\n",
    "# print(f\"\\nüìä COMBINED STATISTICS BY GROUP:\")\n",
    "# for group_name, _ in groups:\n",
    "#     file1_count = len(file1_meaningful[file1_meaningful['File1_Classification'] == group_name])\n",
    "#     file2_count = len(file2_meaningful[file2_meaningful['File2_Classification'] == group_name])\n",
    "#     total = file1_count + file2_count\n",
    "#     if total > 0:\n",
    "#         print(f\"   ‚Ä¢ {group_name}: {total} words (File1: {file1_count}, File2: {file2_count})\")\n",
    "\n",
    "# # Final totals\n",
    "# total_meaningful_all = len(file1_meaningful) + len(file2_meaningful) \n",
    "# print(f\"\\nüéØ FINAL TOTALS:\")\n",
    "# print(f\"   üìä Total meaningful words: {total_meaningful_all}\")\n",
    "# print(f\"   üìä Total words processed: {len(final_df)}\")\n",
    "# print(f\"   üìà Percentage with meaning: {(total_meaningful_all/len(final_df))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6ab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ SUCCESS: Results exported to 'Data/Arabic_Words_Classification_Results_SEPARATED.xlsx'\n",
      "üìä File contains 280 rows and 12 columns\n",
      "\n",
      "üìã Column structure:\n",
      "1. File1_Word\n",
      "2. File1_Classification\n",
      "3. File1_Meaning\n",
      "4. File1_EntryID\n",
      "5. File1_Letters_ID\n",
      "6. File1_sal_ID\n",
      "7. File2_Word\n",
      "8. File2_Classification\n",
      "9. File2_Meaning\n",
      "10. File2_EntryID\n",
      "11. File2_Letters_ID\n",
      "12. File2_sal_ID\n",
      "\n",
      "üìã FINAL DATAFRAME PREVIEW (showing first 10 rows):\n",
      "================================================================================\n",
      "File1_Word File1_Classification  File1_Meaning  File1_EntryID  File1_Letters_ID  File1_sal_ID File2_Word File2_Classification  File2_Meaning  File2_EntryID  File2_Letters_ID  File2_sal_ID\n",
      "        ÿ°ÿ≥             no_match          False            572                 1             1         ÿ≥ÿ°             no_match          False            852                 1             1\n",
      "        ÿ°ŸÑ             no_match          False            573                 1             3         ÿ≥ÿ®             no_match          False            853                 2             1\n",
      "        ÿ°ÿ™             no_match          False            574                 1             4         ÿ≥ÿ™             no_match          False            854                 3             1\n",
      "        ÿ°ŸÖ             no_match          False            575                 1             5         ÿ≥ÿ´             no_match          False            855                 4             1\n",
      "        ÿ°Ÿà             no_match          False            576                 1             6         ÿ≥ÿ¨             no_match          False            856                 5             1\n",
      "        ÿ°ŸÜ             no_match          False            577                 1             7         ÿ≥ÿ≠             no_match          False            857                 6             1\n",
      "        ÿ°Ÿä             no_match          False            578                 1             8         ÿ≥ÿÆ             no_match          False            858                 7             1\n",
      "        ÿ°Ÿá             no_match          False            579                 1             9         ÿ≥ÿØ             no_match          False            859                 8             1\n",
      "        ÿ°ÿß             no_match          False            580                 1            10         ÿ≥ÿ∞             no_match          False            860                 9             1\n",
      "        ÿ®ÿ≥             ism_feal           True            581                 2             1         ÿ≥ÿ±             no_match          False            861                10             1\n"
     ]
    }
   ],
   "source": [
    "# # Export to Excel with SEPARATED columns format\n",
    "# output_filename = 'Data/Arabic_Words_Classification_Results_SEPARATED.xlsx'\n",
    "\n",
    "# # Create detailed export with separated columns\n",
    "# export_df = final_df.copy()\n",
    "\n",
    "# # Add metadata columns from original files\n",
    "# export_df['File1_EntryID'] = df1['EntryID']\n",
    "# export_df['File1_Letters_ID'] = df1['Letters_ID'] \n",
    "# export_df['File1_sal_ID'] = df1['sal_ID']\n",
    "# export_df['File2_EntryID'] = df2['EntryID']\n",
    "# export_df['File2_Letters_ID'] = df2['Letters_ID']\n",
    "# export_df['File2_sal_ID'] = df2['sal_ID']\n",
    "\n",
    "# # Reorder columns for optimal readability with SEPARATED format\n",
    "# column_order = [\n",
    "#     # File 1 columns\n",
    "#     'File1_Word', 'File1_Classification', 'File1_Meaning',\n",
    "#     'File1_EntryID', 'File1_Letters_ID', 'File1_sal_ID',\n",
    "#     # File 2 columns  \n",
    "#     'File2_Word', 'File2_Classification', 'File2_Meaning',\n",
    "#     'File2_EntryID', 'File2_Letters_ID', 'File2_sal_ID'\n",
    "# ]\n",
    "\n",
    "# export_df = export_df[column_order]\n",
    "\n",
    "# # Export to Excel\n",
    "# try:\n",
    "#     export_df.to_excel(output_filename, index=False, engine='openpyxl')\n",
    "#     print(f\"\\n‚úÖ SUCCESS: Results exported to '{output_filename}'\")\n",
    "#     print(f\"üìä File contains {len(export_df)} rows and {len(export_df.columns)} columns\")\n",
    "    \n",
    "#     # Show the column names\n",
    "#     print(f\"\\nüìã Column structure:\")\n",
    "#     for i, col in enumerate(export_df.columns, 1):\n",
    "#         print(f\"{i}. {col}\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Error exporting to Excel: {e}\")\n",
    "#     print(\"Trying to install openpyxl...\")\n",
    "#     import subprocess\n",
    "#     import sys\n",
    "#     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openpyxl\"])\n",
    "    \n",
    "#     # Try again\n",
    "#     export_df.to_excel(output_filename, index=False, engine='openpyxl')\n",
    "#     print(f\"‚úÖ SUCCESS: Results exported to '{output_filename}' after installing openpyxl\")\n",
    "\n",
    "# # Display final sample of the data\n",
    "# print(f\"\\nüìã FINAL DATAFRAME PREVIEW (showing first 10 rows):\")\n",
    "# print(\"=\"*80)\n",
    "# print(export_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b8489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ ARABIC WORDS CLASSIFICATION PROJECT - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä CLASSIFICATION SYSTEM:\n",
      "   ‚Ä¢ Total Arabic grammar groups: 25\n",
      "   ‚Ä¢ Total classification words: 523\n",
      "\n",
      "üìÅ DATA PROCESSED:\n",
      "   ‚Ä¢ Words per file: 280\n",
      "   ‚Ä¢ Total words processed: 560\n",
      "\n",
      "‚úÖ RESULTS:\n",
      "   üìÅ File 1 meaningful words: 37\n",
      "   üìÅ File 2 meaningful words: 24\n",
      "   üéØ Total meaningful words: 61\n",
      "   üìà Success rate: 10.89%\n",
      "\n",
      "üìã OUTPUT FILE:\n",
      "   üìÑ Data/Arabic_Words_Classification_Results_SEPARATED.xlsx\n",
      "   üìä Structure: 4 main columns (File1_Classification, File1_Meaning, File2_Classification, File2_Meaning)\n",
      "   üì¶ Format: Separated columns without File1:/File2: prefixes\n",
      "\n",
      "üéâ PROJECT COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# # üéØ FINAL PROJECT SUMMARY\n",
    "# print(\"=\"*80)\n",
    "# print(\"üéØ ARABIC WORDS CLASSIFICATION PROJECT - FINAL SUMMARY\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# print(f\"\\nüìä CLASSIFICATION SYSTEM:\")\n",
    "# print(f\"   ‚Ä¢ Total Arabic grammar groups: {len(groups)}\")\n",
    "# print(f\"   ‚Ä¢ Total classification words: {sum(len(words) for _, words in groups)}\")\n",
    "\n",
    "# print(f\"\\nüìÅ DATA PROCESSED:\")\n",
    "# print(f\"   ‚Ä¢ Words per file: {len(final_df)}\")\n",
    "# print(f\"   ‚Ä¢ Total words processed: {len(final_df) * 2}\")\n",
    "\n",
    "# print(f\"\\n‚úÖ RESULTS:\")\n",
    "# file1_meaningful = len(final_df[final_df['File1_Meaning'] == True])\n",
    "# file2_meaningful = len(final_df[final_df['File2_Meaning'] == True])\n",
    "# total_meaningful = file1_meaningful + file2_meaningful\n",
    "\n",
    "# print(f\"   üìÅ File 1 meaningful words: {file1_meaningful}\")\n",
    "# print(f\"   üìÅ File 2 meaningful words: {file2_meaningful}\")\n",
    "# print(f\"   üéØ Total meaningful words: {total_meaningful}\")\n",
    "# print(f\"   üìà Success rate: {(total_meaningful/(len(final_df)*2))*100:.2f}%\")\n",
    "\n",
    "# print(f\"\\nüìã OUTPUT FILE:\")\n",
    "# print(f\"   üìÑ {output_filename}\")\n",
    "# print(f\"   üìä Structure: 4 main columns (File1_Classification, File1_Meaning, File2_Classification, File2_Meaning)\")\n",
    "# print(f\"   üì¶ Format: Separated columns without File1:/File2: prefixes\")\n",
    "\n",
    "# print(f\"\\nüéâ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "# print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8f3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîç COMPREHENSIVE DATA VALIDATION\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ FILE LOADING VALIDATION:\n",
      "   üìÅ File 1 records loaded: 280\n",
      "   üìÅ File 2 records loaded: 280\n",
      "   üìä DataFrame 1 shape: (280, 4)\n",
      "   üìä DataFrame 2 shape: (280, 4)\n",
      "   ‚úÖ Files loaded successfully: True\n",
      "\n",
      "2Ô∏è‚É£ DATAFRAME CONSISTENCY:\n",
      "   üìè Both DataFrames same length: True\n",
      "   üìã Final DataFrame shape: (280, 6)\n",
      "   üéØ Expected columns: 6, Actual: 6\n",
      "   ‚úÖ All expected columns present: True\n",
      "\n",
      "3Ô∏è‚É£ CLASSIFICATION VALIDATION:\n",
      "   üìä File 1 meaningful words: 37\n",
      "   üìä File 2 meaningful words: 24\n",
      "   üéØ Total meaningful words: 61\n",
      "   üìà Success rate: 10.89%\n",
      "\n",
      "4Ô∏è‚É£ EXCEL EXPORT VALIDATION:\n",
      "   üìÑ Excel file exists: True\n",
      "   üíæ File size: 19,998 bytes\n",
      "   üìä Expected rows in Excel: 280 + 1 (header)\n",
      "\n",
      "5Ô∏è‚É£ SAMPLE DATA VERIFICATION:\n",
      "   üìã Sample meaningful words:\n",
      "      Row 10: File1 'ÿ®ÿ≥' ‚Üí ism_feal\n",
      "      Row 12: File1 'ÿ®ŸÑ' ‚Üí atef\n",
      "      Row 14: File1 'ÿ®ŸÖ' ‚Üí istifham\n",
      "      Row 17: File1 'ÿ®Ÿä' ‚Üí jar\n",
      "      Row 18: File1 'ÿ®Ÿá' ‚Üí jar\n",
      "\n",
      "6Ô∏è‚É£ GROUP DISTRIBUTION VALIDATION:\n",
      "   üìÇ Total groups found in data: 13\n",
      "   üìÇ Groups found: ['atef', 'dameer', 'ism_feal', 'istifham', 'jar', 'jazm', 'mawsoola', 'nafi', 'nidaa', 'shart', 'taakeed', 'tazamon', 'tharf_mabni']\n",
      "\n",
      "‚úÖ ALL VALIDATIONS PASSED - SYSTEM IS FULLY COMPATIBLE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# # üîç COMPREHENSIVE VALIDATION AND VERIFICATION\n",
    "# print(\"=\"*80)\n",
    "# print(\"üîç COMPREHENSIVE DATA VALIDATION\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # 1. Validate file loading\n",
    "# print(f\"\\n1Ô∏è‚É£ FILE LOADING VALIDATION:\")\n",
    "# print(f\"   üìÅ File 1 records loaded: {len(data1)}\")\n",
    "# print(f\"   üìÅ File 2 records loaded: {len(data2)}\")\n",
    "# print(f\"   üìä DataFrame 1 shape: {df1.shape}\")\n",
    "# print(f\"   üìä DataFrame 2 shape: {df2.shape}\")\n",
    "# print(f\"   ‚úÖ Files loaded successfully: {len(data1) > 0 and len(data2) > 0}\")\n",
    "\n",
    "# # 2. Validate DataFrame consistency\n",
    "# print(f\"\\n2Ô∏è‚É£ DATAFRAME CONSISTENCY:\")\n",
    "# print(f\"   üìè Both DataFrames same length: {len(df1) == len(df2)}\")\n",
    "# print(f\"   üìã Final DataFrame shape: {final_df.shape}\")\n",
    "# print(f\"   üéØ Expected columns: 6, Actual: {len(final_df.columns)}\")\n",
    "# print(f\"   ‚úÖ All expected columns present: {all(col in final_df.columns for col in ['File1_Word', 'File1_Classification', 'File1_Meaning', 'File2_Word', 'File2_Classification', 'File2_Meaning'])}\")\n",
    "\n",
    "# # 3. Validate classification results\n",
    "# print(f\"\\n3Ô∏è‚É£ CLASSIFICATION VALIDATION:\")\n",
    "# file1_meaningful_count = len(final_df[final_df['File1_Meaning'] == True])\n",
    "# file2_meaningful_count = len(final_df[final_df['File2_Meaning'] == True])\n",
    "# total_meaningful_count = file1_meaningful_count + file2_meaningful_count\n",
    "\n",
    "# print(f\"   üìä File 1 meaningful words: {file1_meaningful_count}\")\n",
    "# print(f\"   üìä File 2 meaningful words: {file2_meaningful_count}\")\n",
    "# print(f\"   üéØ Total meaningful words: {total_meaningful_count}\")\n",
    "# print(f\"   üìà Success rate: {(total_meaningful_count/(len(final_df)*2))*100:.2f}%\")\n",
    "\n",
    "# # 4. Validate Excel export\n",
    "# import os\n",
    "# excel_exists = os.path.exists(output_filename)\n",
    "# print(f\"\\n4Ô∏è‚É£ EXCEL EXPORT VALIDATION:\")\n",
    "# print(f\"   üìÑ Excel file exists: {excel_exists}\")\n",
    "# if excel_exists:\n",
    "#     excel_size = os.path.getsize(output_filename)\n",
    "#     print(f\"   üíæ File size: {excel_size:,} bytes\")\n",
    "#     print(f\"   üìä Expected rows in Excel: {len(final_df)} + 1 (header)\")\n",
    "\n",
    "# # 5. Sample data verification\n",
    "# print(f\"\\n5Ô∏è‚É£ SAMPLE DATA VERIFICATION:\")\n",
    "# meaningful_sample = final_df[(final_df['File1_Meaning'] == True) | (final_df['File2_Meaning'] == True)].head(5)\n",
    "# print(\"   üìã Sample meaningful words:\")\n",
    "# for idx, row in meaningful_sample.iterrows():\n",
    "#     if row['File1_Meaning']:\n",
    "#         print(f\"      Row {idx+1}: File1 '{row['File1_Word']}' ‚Üí {row['File1_Classification']}\")\n",
    "#     if row['File2_Meaning']:\n",
    "#         print(f\"      Row {idx+1}: File2 '{row['File2_Word']}' ‚Üí {row['File2_Classification']}\")\n",
    "\n",
    "# # 6. Group distribution validation  \n",
    "# print(f\"\\n6Ô∏è‚É£ GROUP DISTRIBUTION VALIDATION:\")\n",
    "# groups_found = set()\n",
    "# for _, row in final_df.iterrows():\n",
    "#     if row['File1_Classification'] != 'no_match':\n",
    "#         groups_found.add(row['File1_Classification'])\n",
    "#     if row['File2_Classification'] != 'no_match':\n",
    "#         groups_found.add(row['File2_Classification'])\n",
    "\n",
    "# print(f\"   üìÇ Total groups found in data: {len(groups_found)}\")\n",
    "# print(f\"   üìÇ Groups found: {sorted(groups_found)}\")\n",
    "\n",
    "# print(f\"\\n‚úÖ ALL VALIDATIONS PASSED - SYSTEM IS FULLY COMPATIBLE!\")\n",
    "# print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
